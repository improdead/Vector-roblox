/**
 * AI-Based Code Review for Intelligence Tests
 *
 * Uses GPT-4o-mini to evaluate the agent's output quality for intelligence/reasoning tests.
 * Reviews code structure, planning, adherence to best practices, and Roblox-specific criteria.
 *
 * @module testing/runner/ai-reviewer
 */

import { ExecutionResult } from './agent-executor';

/**
 * AI review result
 */
export interface AIReviewResult {
  passed: boolean;
  score: number; // 0-100
  issues: string[];
  insights: string[];
  reasoning: string;
}

/**
 * Review criteria for Roblox code quality
 */
const ROBLOX_QUALITY_CRITERIA = `
## Roblox Code Quality Criteria

You are reviewing Luau code generated by Vector, a Roblox Studio AI copilot. Evaluate the output based on:

### 1. Idempotency and Safety
- Scripts MUST check for existing objects before creating (FindFirstChild, if not exists)
- Scripts MUST clean up old objects before recreating (remove old, then create new)
- No duplicate creation without checking first
- Example GOOD pattern:
  \`\`\`lua
  local existingPart = workspace:FindFirstChild("MyPart")
  if existingPart then
    existingPart:Destroy()
  end
  local part = Instance.new("Part")
  part.Name = "MyPart"
  part.Parent = workspace
  \`\`\`

### 2. Anchoring and Physics
- All parts that should stay in place MUST have Anchored = true
- Parts without explicit behavior should default to Anchored = true
- Only leave unanchored if physics simulation is explicitly intended

### 3. Positioning and Sizing
- MUST use CFrame.new() or CFrame.Angles() for positioning and rotation
- MUST use Vector3.new() for Size
- Coordinates should be reasonable (not all at 0,0,0)
- Grid structures should have proper spacing calculations

### 4. Code Structure
- Loops for repetitive tasks (grids, arrays)
- No hardcoded repetition (copy-paste)
- Clear variable names
- Proper hierarchy (Models containing Parts)

### 5. Luau Best Practices
- Use Instance.new() properly
- Set properties before setting Parent (performance)
- Use Color3.fromRGB() or Color3.new() for colors
- Use proper Material enums

### 6. Script Policy Compliance
- Vector has a "Script Policy": ANY geometry changes should be scripted in Luau
- The agent should write Luau code even for simple geometry
- Only skip scripting if user explicitly opts out ("no code", "geometry only")

### 7. Asset-First Approach
- For complex objects (buildings, vehicles, furniture), prefer search_assets over manual geometry
- Only use create_instance for simple primitives (cubes, spheres, cylinders)
- Avoid building complex structures manually when catalog search could work

## Scoring Guidelines

**90-100 (Excellent)**
- All criteria met
- Idempotent code with proper checks
- Clean structure with loops
- Proper anchoring, sizing, positioning
- Follows script policy and asset-first approach

**70-89 (Good)**
- Most criteria met
- Minor issues (e.g., missing some anchors, suboptimal structure)
- Generally correct but could be improved

**50-69 (Acceptable)**
- Basic functionality present
- Some significant issues (e.g., no idempotency checks, poor structure)
- Works but not production-quality

**Below 50 (Poor)**
- Major issues
- Missing critical features (no idempotency, duplicate creation)
- Does not follow basic best practices
- Script policy violations

## What to Look For

**RED FLAGS:**
- Creating objects without checking if they exist (not idempotent)
- Parts not anchored when they should be
- All positions at (0,0,0)
- No loops for repetitive tasks
- Manual geometry for complex objects that should use assets
- No Luau script when geometry was created (script policy violation)

**POSITIVE SIGNALS:**
- FindFirstChild checks before creation
- Proper CFrame calculations for positioning
- Loops for grids/arrays
- Anchored = true for static geometry
- Clear planning and reasoning
- Asset search before manual building
`;

/**
 * Vector's core system prompt excerpt (for context)
 */
const VECTOR_SYSTEM_EXCERPT = `
You are Vector, a Roblox Studio copilot.

Core rules:
- One tool per turn: emit EXACTLY ONE tool tag
- Proposal-first and undoable: never change code/Instances outside a tool
- Plan when work spans multiple steps

Default Script Policy:
- Whenever you create, modify, or insert Instances, you must author Luau that rebuilds the result before completing
- Preferred flow: open_or_create_script → show_diff (or apply_edit when already previewed)
- Scripts must be valid, idempotent, and set Anchored/props explicitly
- Skip Luau only when the user explicitly opts out

Assets & 3D:
- Prefer search_assets → insert_asset for props/models
- Use create_instance only for simple primitive geometry or when catalog search fails
- Before creating or inserting, inspect existing children and skip duplicates

Scene building:
- Always think through the layout before acting: use <start_plan>
- Inspect what already exists
- Build geometry iteratively with create_instance/set_properties
`;

/**
 * Review prompt template
 */
function buildReviewPrompt(result: ExecutionResult): string {
  const { proposals, finalState, toolCalls } = result;

  // Extract script content if available
  let scriptContent = '';
  const scriptFiles = finalState.files.filter(([path, _]) =>
    path.endsWith('.lua') || path.endsWith('.luau')
  );
  if (scriptFiles.length > 0) {
    scriptContent = scriptFiles.map(([path, file]) => `
### Script: ${path}
\`\`\`lua
${file.content}
\`\`\`
`).join('\n');
  }

  // Extract instance operations
  let instanceOps = '';
  const objectProposals = proposals.filter(p => p.type === 'object_op');
  if (objectProposals.length > 0) {
    instanceOps = objectProposals.map(p => {
      const ops = (p as any).ops || [];
      return ops.map((op: any) => `- ${op.op}: ${JSON.stringify(op)}`).join('\n');
    }).join('\n');
  }

  // Extract tool calls summary
  const toolSummary = toolCalls.map(tc => `- ${tc.tool}`).join('\n');

  return `
You are reviewing the output of Vector, a Roblox Studio AI copilot, for a test scenario.

## Vector's Role
${VECTOR_SYSTEM_EXCERPT}

## Quality Criteria
${ROBLOX_QUALITY_CRITERIA}

## Test Execution Summary

**User Request:** ${result.taskState.userMessage || 'Not available'}

**Tool Calls Made:**
${toolSummary}

**Script Content:**
${scriptContent || 'No scripts generated'}

**Instance Operations:**
${instanceOps || 'No direct instance operations'}

**Final State:**
- Files: ${finalState.files.length}
- Instances: ${finalState.instances.length}

## Your Task

Review the agent's output and provide:

1. **Score (0-100)**: Based on the scoring guidelines above
2. **Passed (true/false)**: Score >= 70 is passing
3. **Issues**: List of specific problems or violations (if any)
4. **Insights**: Positive observations or suggestions for improvement
5. **Reasoning**: Brief explanation of your evaluation

Focus on:
- Does the code follow Roblox best practices?
- Is the script idempotent (checks before creating)?
- Are parts properly anchored and positioned?
- Does it follow Vector's script policy?
- Is the structure clean and maintainable?

Respond in JSON format:
\`\`\`json
{
  "score": 85,
  "passed": true,
  "issues": ["List of issues"],
  "insights": ["List of positive observations"],
  "reasoning": "Brief explanation"
}
\`\`\`
`;
}

/**
 * Call OpenAI API for AI review
 */
async function callOpenAI(prompt: string): Promise<any> {
  const apiKey = process.env.REVIEWER_OPENAI_API_KEY || process.env.OPENAI_API_KEY;
  const model = process.env.REVIEWER_MODEL || 'gpt-4o-mini';

  if (!apiKey) {
    throw new Error('REVIEWER_OPENAI_API_KEY or OPENAI_API_KEY not set in .env');
  }

  const response = await fetch('https://api.openai.com/v1/chat/completions', {
    method: 'POST',
    headers: {
      'Content-Type': 'application/json',
      'Authorization': `Bearer ${apiKey}`,
    },
    body: JSON.stringify({
      model,
      messages: [
        {
          role: 'system',
          content: 'You are a code reviewer for Roblox Luau scripts. You provide structured, objective feedback based on best practices and quality criteria. Always respond with valid JSON.'
        },
        {
          role: 'user',
          content: prompt
        }
      ],
      temperature: 0.3, // Lower temperature for more consistent reviews
      max_tokens: 1000,
    }),
  });

  if (!response.ok) {
    const errorText = await response.text();
    throw new Error(`OpenAI API error: ${response.status} ${errorText}`);
  }

  const data = await response.json();
  const content = data.choices[0]?.message?.content;

  if (!content) {
    throw new Error('No content in OpenAI response');
  }

  // Extract JSON from code fence if present
  let jsonStr = content;
  const jsonMatch = content.match(/```json\s*\n?([\s\S]*?)\n?```/);
  if (jsonMatch) {
    jsonStr = jsonMatch[1];
  }

  try {
    return JSON.parse(jsonStr);
  } catch (e) {
    throw new Error(`Failed to parse OpenAI response as JSON: ${e}\nContent: ${content}`);
  }
}

/**
 * Review agent output using AI
 */
export async function reviewWithAI(result: ExecutionResult): Promise<AIReviewResult> {
  const prompt = buildReviewPrompt(result);

  try {
    const review = await callOpenAI(prompt);

    return {
      passed: review.passed === true,
      score: Number(review.score) || 0,
      issues: Array.isArray(review.issues) ? review.issues : [],
      insights: Array.isArray(review.insights) ? review.insights : [],
      reasoning: String(review.reasoning || ''),
    };
  } catch (error) {
    // If AI review fails, return a neutral result
    console.warn('[ai-reviewer] Review failed:', error);
    return {
      passed: true, // Don't fail tests due to reviewer errors
      score: 0,
      issues: [`AI review failed: ${error}`],
      insights: [],
      reasoning: 'AI reviewer encountered an error',
    };
  }
}

/**
 * Check if AI review is enabled
 */
export function isAIReviewEnabled(): boolean {
  return !!(process.env.REVIEWER_OPENAI_API_KEY || process.env.OPENAI_API_KEY);
}
